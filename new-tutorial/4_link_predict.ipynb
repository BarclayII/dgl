{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction using Graph Neural Networks\n",
    "\n",
    "In the [introduction](1_introduction.ipynb), you already learned the basic workflow of using GNNs for node classification, i.e. predicting the category of a node in a graph.  This tutorial will teach you how to train a GNN for link prediction, i.e. predicting the existence of an edge between two arbitrary nodes in a graph.\n",
    "\n",
    "By the end of this tutorial you will be able to\n",
    "\n",
    "* Build a GNN-based link prediction model.\n",
    "* Train and evaluate the model on a small DGL-provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Link Prediction with GNN\n",
    "\n",
    "Many applications such as social recommendation, item recommendation, knowledge graph completion, etc., can be formulated as link prediction, which predicts whether an edge exists between two particular nodes.  This tutorial shows an example of predicting whether a citation relationship, either citing or being cited, between two papers exists in a citation network.\n",
    "\n",
    "This tutorial follows a relatively simple practice from [SEAL](https://papers.nips.cc/paper/2018/file/53f0d7c537d99b3824f0f99d62ea2428-Paper.pdf).  It formulates the link prediction problem as a binary classification problem as follows:\n",
    "\n",
    "* Treat the edges in the graph as *positive examples*.\n",
    "* Sample a number of non-existent edges (i.e. node pairs with no edges between them) as *negative* examples.\n",
    "* Divide the positive examples and negative examples into a training set and a test set.\n",
    "* Evaluate the model with any binary classification metric such as Area Under Curve (AUC).\n",
    "\n",
    "In some domains such as large-scale recommender systems or information retrieval, you may favor metrics that emphasize good performance of top-K predictions.  In these cases you may want to consider other metrics such as mean average precision, and use other negative sampling methods, which are beyond the scope of this tutorial.\n",
    "\n",
    "## Load graph and features\n",
    "\n",
    "Following the [introduction](1_introduction.ipynb), we first load the Cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and testing sets\n",
    "\n",
    "This tutorial randomly picks 10% of the edges for positive examples in the test set, and leave the rest for the training set.  It then samples the same number of edges for negative examples in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "u, v = g.edges()\n",
    "\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.1)\n",
    "train_size = g.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all negative edges and split them for training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "neg_eids = np.random.choice(len(neg_u), g.number_of_edges() // 2)\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set.\n",
    "train_u = torch.cat([torch.as_tensor(train_pos_u), torch.as_tensor(train_neg_u)])\n",
    "train_v = torch.cat([torch.as_tensor(train_pos_v), torch.as_tensor(train_neg_v)])\n",
    "train_label = torch.cat([torch.zeros(len(train_pos_u)), torch.ones(len(train_neg_u))])\n",
    "\n",
    "# Create testing set.\n",
    "test_u = torch.cat([torch.as_tensor(test_pos_u), torch.as_tensor(test_neg_u)])\n",
    "test_v = torch.cat([torch.as_tensor(test_pos_v), torch.as_tensor(test_neg_v)])\n",
    "test_label = torch.cat([torch.zeros(len(test_pos_u)), torch.ones(len(test_neg_u))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training, you will need to remove the edges in the test set from the original graph.  You can do this via `dgl.remove_edges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = dgl.remove_edges(g, eids[:test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a GraphSAGE model\n",
    "\n",
    "This tutorial builds a model consisting of two [GraphSAGE](https://arxiv.org/abs/1706.02216) layers, each computes new node representations by averaging neighbor information.  DGL provides `dgl.nn.SAGEConv` that conveniently creates a GraphSAGE layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "model = GraphSAGE(train_g.ndata['feat'].shape[1], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model then predicts the probability of existence of an edge by computing a dot product between the representations of both incident nodes.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{u\\sim v} = \\sigma(h_u^T h_v)\n",
    "$$\n",
    "\n",
    "The loss function is simply binary cross entropy loss.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\n",
    "$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: this tutorial did not include evaluation on a validation set.  In practice you should save and evaluate the best model based on performance on the validation set.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.8055661916732788\n",
      "In epoch 5, loss: 0.7043412923812866\n",
      "In epoch 10, loss: 0.6985824704170227\n",
      "In epoch 15, loss: 0.6954853534698486\n",
      "In epoch 20, loss: 0.6935545802116394\n",
      "In epoch 25, loss: 0.6914740204811096\n",
      "In epoch 30, loss: 0.68902987241745\n",
      "In epoch 35, loss: 0.6853228211402893\n",
      "In epoch 40, loss: 0.6779254078865051\n",
      "In epoch 45, loss: 0.6644536256790161\n",
      "In epoch 50, loss: 0.6398479342460632\n",
      "In epoch 55, loss: 0.6043607592582703\n",
      "In epoch 60, loss: 0.5700518488883972\n",
      "In epoch 65, loss: 0.5423790216445923\n",
      "In epoch 70, loss: 0.5080265402793884\n",
      "In epoch 75, loss: 0.47435256838798523\n",
      "In epoch 80, loss: 0.4486607313156128\n",
      "In epoch 85, loss: 0.42552614212036133\n",
      "In epoch 90, loss: 0.4029264450073242\n",
      "In epoch 95, loss: 0.38153502345085144\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(itertools.chain(model.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    logits = model(train_g, train_g.ndata['feat'])\n",
    "    pred = torch.sigmoid((logits[train_u] * logits[train_v]).sum(dim=1))\n",
    "    \n",
    "    # compute loss\n",
    "    loss = F.binary_cross_entropy(pred, train_label)\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.6724574919700815\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "from sklearn.metrics import roc_auc_score\n",
    "with torch.no_grad():\n",
    "    pred = torch.sigmoid((logits[test_u] * logits[test_v]).sum(dim=1))\n",
    "    pred = pred.numpy()\n",
    "    label = test_label.numpy()\n",
    "    print('AUC', roc_auc_score(label, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "* See [here](L2_large_link_prediction.ipynb) for a tutorial on link prediction on a large-scale graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
