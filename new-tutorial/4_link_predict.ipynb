{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link Prediction using Graph Neural Networks\n",
    "\n",
    "This tutorial teaches the basic workflow of using GNNs for link prediction, i.e. predicting whether an edge exist between two nodes. This tutorial again uses the Cora dataset but try to predict interactions (citation relationships) between two papers in a graph.\n",
    "\n",
    "Goal of this tutorial:\n",
    "\n",
    "* Prepare training and testing sets for link prediction task.\n",
    "* Build a GNN-based link prediction model.\n",
    "* Train the model and verify the result.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note: </b>The Cora dataset provided by DGL is bidirectional, meaning that the edges can only represent whether a citation relationship exist between two papers: they cannot tell which paper cites which other paper.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph and features\n",
    "\n",
    "The dataset used in the tutorial will still be Cora following the [introduction](1_introduction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training and testing sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a link prediction data set contains two types of edges, *positive* and *negative edges*. Positive edges are usually drawn from the existing edges in the graph. This tutorial randomly picks 1000 edges for testing and leaves the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "TEST_SIZE = 1000\n",
    "u, v = g.edges()\n",
    "eids = np.arange(g.num_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_eids = eids[:TEST_SIZE]\n",
    "train_eids = eids[TEST_SIZE:]\n",
    "test_u, test_v = u[test_eids], v[test_eids]\n",
    "train_u, train_v = u[train_eids], v[train_eids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because you will be predicting whether an edge in the test set exists on a graph, the test edges should not exist in the training graph.  We therefore need to train on a graph only consisting of the edges in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = g.edge_subgraph(train_eids, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a GraphSAGE model\n",
    "\n",
    "Our model will be a two-layer [GraphSAGE convolution (Hamilton et al., 2017)](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf).  DGL supports, alongside GraphSAGE via [`dgl.nn.SAGEConv`](https://docs.dgl.ai/api/python/nn.pytorch.html#sageconv), [many other graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# Create the model with hidden layer dimension 16.\n",
    "net = GraphSAGE(g.ndata['feat'].shape[1], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then optimize the model using the following loss function.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{u\\to v} = \\sigma(h_u^T h_v)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{u\\to v\\in \\mathcal{D}}\\left( y_{u\\to v}\\log(\\hat{y}_{u\\to v}) + (1-y_{u\\to v})\\log(1-\\hat{y}_{u\\to v})) \\right)\n",
    "$$\n",
    "\n",
    "Essentially, to predict whether an edge exists between two nodes, the model predicts a score by computing a dot product between both nodes' representations.  The model then minimizes the loss function above so that node pairs that have an edge (or *positive examples*) in between get a higher score, while the other node pairs that do not have an edge in between (or *negative examples*) get a lower score.\n",
    "\n",
    "Since the number of possible node pairs is large, one often samples a small number of node pairs and computes loss only on the sampled node pairs instead.  This is called *negative sampling*.  Here we simply pick the node pairs uniformly; more sophisticated negative sampling strategies are beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_examples(g):\n",
    "    # Randomly pick as many negative examples as the positive examples\n",
    "    # (i.e. all edges in the training graph).\n",
    "    neg_u = torch.randint(0, g.num_nodes(), (g.num_edges(),))\n",
    "    neg_v = torch.randint(0, g.num_nodes(), (g.num_edges(),))\n",
    "    return neg_u, neg_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.715376615524292\n",
      "In epoch 5, loss: 0.6942322850227356\n",
      "In epoch 10, loss: 0.6904299855232239\n",
      "In epoch 15, loss: 0.683499276638031\n",
      "In epoch 20, loss: 0.6637445092201233\n",
      "In epoch 25, loss: 0.6305882334709167\n",
      "In epoch 30, loss: 0.5955677032470703\n",
      "In epoch 35, loss: 0.5764950513839722\n",
      "In epoch 40, loss: 0.5546678900718689\n",
      "In epoch 45, loss: 0.5326592922210693\n",
      "In epoch 50, loss: 0.5074127912521362\n",
      "In epoch 55, loss: 0.47796398401260376\n",
      "In epoch 60, loss: 0.451864093542099\n",
      "In epoch 65, loss: 0.42299002408981323\n",
      "In epoch 70, loss: 0.3948816955089569\n",
      "In epoch 75, loss: 0.36992791295051575\n",
      "In epoch 80, loss: 0.3451216220855713\n",
      "In epoch 85, loss: 0.32089129090309143\n",
      "In epoch 90, loss: 0.2978387773036957\n",
      "In epoch 95, loss: 0.27626192569732666\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# in this case, loss will in training loop\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # Forward computation that computes output embedding of the nodes\n",
    "    h = net(g, g.ndata['feat'])\n",
    "    \n",
    "    neg_u, neg_v = generate_negative_examples(g)\n",
    "    pred_pos = (h[train_u] * logits[train_v]).sum(dim=1)\n",
    "    pred_neg = (h[neg_u] * h[neg_v]).sum(dim=1)\n",
    "    label_pos = torch.ones_like(pred_pos)\n",
    "    label_neg = torch.zeros_like(pred_neg)\n",
    "    pred = torch.cat([pred_pos, pred_neg])\n",
    "    label = torch.cat([label_pos, label_neg])\n",
    "    # compute loss\n",
    "    loss = F.binary_cross_entropy(pred, label)\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7495\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "pred = torch.sigmoid((logits[test_u] * logits[test_v]).sum(dim=1))\n",
    "print('Accuracy', ((pred >= 0.5) == test_label).sum().item() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "If you wish to scale up your link prediction model, please see the tutorial [Stochastic Training of GNN for Link Prediction on Large Graphs](L2_large_link_prediction.ipynb).\n",
    "* The training experience on large graph is different from training on full graphs, so we recommend you to go through the tutorial [Stochastic Training of GNN for Node Classification on Large Graphs](L1_large_node_classification.ipynb) first to get an idea of how large graph training works.\n",
    "\n",
    "If you have heterogeneous graphs, please see the tutorial [Link Prediction on Heterogeneous Graphs (TODO)](H4_link_predict.ipynb).\n",
    "* We recommend you to go through the tutorial [Node Classification on Heterogeneous Graphs (TODO)](H1_node_classification.ipynb) to get an idea of how heterogeneous graph training works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
