{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing GNN Modules for Stochastic GNN Training\n",
    "\n",
    "All GNN modules DGL provides support stochastic GNN training.  This tutorial teaches you how to write your own graph neural network module for stochastic GNN training.  It assumes that\n",
    "\n",
    "1. You know [how to write GNN modules for full graph training](3_message_passing.ipynb).\n",
    "2. You know [how stochastic GNN training pipeline works](L1_large_node_classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-products')\n",
    "\n",
    "graph, node_labels = dataset[0]\n",
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "node_features = graph.ndata['feat']\n",
    "\n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "input_nodes, output_nodes, bipartites = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGL Bipartite Graph Introduction\n",
    "\n",
    "In the previous tutorials such as [node classification](L1_large_node_classification.ipynb), [link prediction](L2_large_link_prediction.ipynb), and [custom neighbor sampling](L3_custom_sampler.ipynb), you have seen the concept *bipartite graph*.  This section introduces how you can manipulate (directional) bipartite graphs.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "     \n",
    "**Question**: shall we tell the users how to create a bipartite graph here?  We don't have a `dgl.bipartite()` interface, so a user must dive straight into the \"heterogeneous graph\" concept.\n",
    "     \n",
    "</div>\n",
    "\n",
    "You can access the input node features and output node features via `srcdata` and `dstdata` attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feat': tensor([[-0.0808,  0.5835, -1.1753,  ...,  1.0232, -0.4817,  2.8607],\n",
      "        [ 0.7227, -0.1247, -0.0356,  ...,  0.6085,  0.7714, -0.2870],\n",
      "        [ 0.1973,  0.0741, -0.0163,  ...,  0.6373,  0.0277,  0.1868],\n",
      "        ...,\n",
      "        [-0.0392, -0.2355,  0.1328,  ..., -0.3510,  0.5908,  0.9608],\n",
      "        [ 0.1111,  0.3545,  0.0535,  ...,  0.1027,  1.1263,  0.6594],\n",
      "        [ 0.0189,  0.4955,  0.1431,  ..., -0.3645,  0.4985,  0.7945]]), '_ID': tensor([195412,  60541,  49188,  ...,  63542,  44467,  81629])}\n",
      "{'feat': tensor([[-0.0808,  0.5835, -1.1753,  ...,  1.0232, -0.4817,  2.8607],\n",
      "        [ 0.7227, -0.1247, -0.0356,  ...,  0.6085,  0.7714, -0.2870],\n",
      "        [ 0.1973,  0.0741, -0.0163,  ...,  0.6373,  0.0277,  0.1868],\n",
      "        ...,\n",
      "        [ 0.2732, -0.8037,  0.1437,  ..., -0.4809,  0.9125,  0.9294],\n",
      "        [-0.6513,  0.0445,  0.0212,  ..., -0.6451,  0.7263,  0.7248],\n",
      "        [ 0.4365,  0.0622,  0.3198,  ..., -0.3303,  0.0735,  0.2900]]), '_ID': tensor([195412,  60541,  49188,  ..., 157118,  35460, 802935])}\n"
     ]
    }
   ],
   "source": [
    "bipartite = bipartites[0]\n",
    "print(bipartite.srcdata)\n",
    "print(bipartite.dstdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also has `num_src_nodes` and `num_dst_nodes` functions to query how many input nodes and output nodes exist in the bipartite graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23865 5086\n"
     ]
    }
   ],
   "source": [
    "print(bipartite.num_src_nodes(), bipartite.num_dst_nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can assign features to `srcdata` and `dstdata` just as what you will do with `ndata` on the graphs you have seen earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite.srcdata['x'] = torch.zeros(bipartite.num_src_nodes(), bipartite.num_dst_nodes())\n",
    "dst_feat = bipartite.dstdata['feat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since the bipartite graphs are constructed by DGL, you can retrieve the input node IDs (i.e. those that are required to compute the output) and output node IDs (i.e. those whose representations the current GNN layer should compute) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([195412,  60541,  49188,  ...,  63542,  44467,  81629]),\n",
       " tensor([195412,  60541,  49188,  ..., 157118,  35460, 802935]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bipartite.srcdata[dgl.NID], bipartite.dstdata[dgl.NID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing GNN Modules for Bipartite Graphs for Stochastic Training\n",
    "\n",
    "Recall from the [custom message passing tutorial for small graphs](3_message_passing.ipynb) that the message passing formulation in [Glimer et al.](https://arxiv.org/abs/1704.01212) works as follows:\n",
    "\n",
    "$$\n",
    "m_{u\\to v}^{(l)} = M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\to v}^{(l-1)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "m_{v}^{(l)} = \\sum_{u\\in\\mathcal{N}(v)}m_{u\\to v}^{(l)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^{(l)} = U^{(l)}\\left(h_v^{(l-1)}, m_v^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "where $M$ can be any message function, $\\sum$ can be any reduce function, and $U$ can be any function that combines the message aggregation and the representation of node $v$ itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, recall that the bipartite graphs yielded by the `NodeDataLoader` and `EdgeDataLoader` have the property that the first few input nodes are always identical to the output nodes:\n",
    "\n",
    "![](assets/bipartite2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.equal(bipartite.srcdata[dgl.NID][:bipartite.num_dst_nodes()], bipartite.dstdata[dgl.NID]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you have obtained the input node representations $h_u^{(l-1)}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bipartite.srcdata['h'] = torch.randn(bipartite.num_src_nodes(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the input nodes is a union of the output nodes and their neighbors, enabling you to conveniently get the term $h_v^{(l-1)}$ via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_v = bipartite.srcdata['h'][:bipartite.num_dst_nodes()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that the message function is simply copying the source feature (i.e. $M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\to v}^{(l-1)}\\right) = h_v^{(l-1)}$), and the reduce function is simply average, you can still use `update_all` to compute $m_{v}^{(l)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0733, -0.2727,  0.2788,  ...,  0.4624,  0.0740,  0.6660],\n",
       "        [ 0.1595,  0.6646, -0.8184,  ..., -0.7316, -1.1893, -0.7929],\n",
       "        [-0.2563,  0.1930,  0.0794,  ..., -0.4013,  0.5718, -0.0207],\n",
       "        ...,\n",
       "        [-0.9491,  0.1209, -0.6648,  ...,  0.2813, -1.0527,  1.0429],\n",
       "        [-0.0577,  0.3248,  0.2989,  ...,  0.0615, -0.3329,  0.2392],\n",
       "        [ 0.0766,  0.1231, -0.7424,  ..., -0.4826,  0.4557, -0.3635]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "bipartite.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h'))\n",
    "m_v = bipartite.dstdata['h']\n",
    "m_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting them together, you can implement a GraphSAGE convolution for large graph training as follows (the differences to the [small graph counterpart](3_message_passing.ipynb) are highlighted with arrows)\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Question**: do we still suggest users to slice\n",
    "    \n",
    "```python\n",
    "h_dst = h[:g.num_dst_nodes()]\n",
    "```\n",
    "    \n",
    "within the NN module?\n",
    "    \n",
    "* If so, we will still need to have an `is_block` flag which is set from `to_block`.  Otherwise the DGL NN module will never know whether it should perform the slicing or not: it can be both a normal bipartite graph (which doesn't require slicing) or something returned by neighbor sampler (which requires slicing).\n",
    "* If not, then it will break existing code, and lots of examples need to change.  Moreover, doing so for heterogeneous graph is a bit heavy as you will need something like:\n",
    "\n",
    "  ```python\n",
    "  h_dst = {ntype: h[:g.num_dst_nodes(ntype)] for ntype in g.ntypes if ntype.endswith('_dst')}\n",
    "  ```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:07<00:00, 24.59it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule for projecting the input and neighbor feature to the output.\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        \"\"\"Forward computation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        g : Graph\n",
    "            The input bipartite graph.\n",
    "        h : Tensor\n",
    "            The input node feature.\n",
    "        \"\"\"\n",
    "        with g.local_scope():\n",
    "            h_dst = h[:g.num_dst_nodes()]                 # <---\n",
    "            g.srcdata['h'] = h                            # <---\n",
    "            # update_all is a message passing API.\n",
    "            g.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_neigh'))\n",
    "            h_neigh = g.dstdata['h_neigh']\n",
    "            h_total = torch.cat([h_dst, h_neigh], dim=1)  # <---\n",
    "            return self.linear(h_total)\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes)\n",
    "    \n",
    "    def forward(self, bipartites, in_feat):\n",
    "        h = self.conv1(bipartites[0], in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(bipartites[1], h)\n",
    "        return h\n",
    "    \n",
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "model = Model(graph.ndata['feat'].shape[1], 128, dataset.num_classes).cuda()\n",
    "\n",
    "with tqdm.tqdm(train_dataloader) as tq:\n",
    "    for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "        bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "        inputs = node_features[input_nodes].cuda()\n",
    "        labels = node_labels[output_nodes].cuda()\n",
    "        predictions = model(bipartites, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `update_all` and the functions in `nn.functional` namespace support bipartite graphs, so you can migrate the code working for small graphs to large graph training with minimal changes introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
