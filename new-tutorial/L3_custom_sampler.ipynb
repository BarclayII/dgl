{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizing Neighborhood Sampling\n",
    "\n",
    "This tutorial will teach you how to specify your own neighbor sampling algorithm in large scale graph training.  It assumes that you have read the basics of large graph training in the [node classification tutorial](L1_large_node_classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-products')\n",
    "\n",
    "graph, node_labels = dataset[0]\n",
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of large graph training pipeline\n",
    "\n",
    "Training a GNN on a single large graph involves the following steps:\n",
    "\n",
    "1. Iterate over the nodes or edges in minibatches.\n",
    "2. For each minibatch, generate a list of bipartite graphs, one bipartite graph for each GNN layer.\n",
    "3. Perform message passing on the list of bipartite graphs.\n",
    "4. Compute gradient and optimize.\n",
    "\n",
    "Neighborhood sampling customization takes place in Step 2.\n",
    "\n",
    "## How neighbor sampling works in DGL\n",
    "\n",
    "Uniform neighbor sampling in DGL as mentioned in the [node classification tutorial](L1_large_node_classification.ipynb) is simply implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, fanouts):\n",
    "        super().__init__(len(fanouts), return_eids=False)\n",
    "        self.fanouts = fanouts\n",
    "        \n",
    "    def sample_frontier(self, layer_id, g, seed_nodes):\n",
    "        fanout = self.fanouts[layer_id]\n",
    "        return dgl.sampling.sample_neighbors(g, seed_nodes, fanout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `NeighborSampler` inherits the `dgl.dataloading.BlockSampler` class.  The core method is `sample_frontier`, where it receives three arguments:\n",
    "\n",
    "* `layer_id`: Indicating which GNN layer the neighbor sampler is processing.  0 indicates the GNN layer closest to the input.  DGL iterates from the last GNN layer to the first GNN layer as introduced in the [node classification tutorial](L1_large_node_classification.ipynb).\n",
    "* `g`: The entire graph.  It can contain node and edge features so that the neighbor sampler can utilize them.\n",
    "* `seed_nodes`: The array of node IDs whose output representations the given GNN layer should compute.\n",
    "\n",
    "It returns one single object, which is a graph containing all the nodes in the original graph, as well as the edges for the GNN layer to perform message passing to compute the outputs of the `seed_nodes`.\n",
    "\n",
    "`MultiLayerNeighborSampler` does this via `dgl.sampling.sample_neighbors` that returns a subgraph of the original graph `g`.  It includes all the nodes in `g`, as well as the randomly sampled, fixed number of incoming edges for each node in the ID array `seed_nodes`.\n",
    "\n",
    "The inheritance of `dgl.dataloading.BlockSampler` would transform the graph returned by `sample_frontier` into a bipartite graph:\n",
    "\n",
    "![](assets/bipartite.png)\n",
    "\n",
    "The transformation in `dgl.dataloading.BlockSampler` ensures that the first few input nodes are always the same as the output nodes, even if some of the output nodes did not appear as a neighbor of any output node:\n",
    "\n",
    "![](assets/bipartite2.png)\n",
    "\n",
    "For more details, please see the [message passing tutorial](L4_message_passing.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing Neighbor Sampler\n",
    "\n",
    "This tutorial gives two examples of customized neighbor sampling:\n",
    "\n",
    "### Message dropout\n",
    "\n",
    "One example is to randomly drop the neighboring edges with a fixed probability.  You can do that via returning a subgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageDropoutNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, p, num_layers):\n",
    "        super().__init__(num_layers, return_eids=False)\n",
    "        self.p = p\n",
    "        \n",
    "    def sample_frontier(self, layer_id, g, seed_nodes):\n",
    "        sg = dgl.in_subgraph(g, seed_nodes)\n",
    "        eids = sg.edata[dgl.EID]\n",
    "        mask = torch.zeros(len(eids), dtype=torch.bool).bernoulli_(self.p)\n",
    "        sampled_eids = eids[mask]\n",
    "        return dgl.edge_subgraph(g, sampled_eids, preserve_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = MessageDropoutNeighborSampler(0.5, 2)\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "input_nodes, output_nodes, bipartites = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling k-hop neighbors\n",
    "\n",
    "Another commonly seen scenario is to directly aggregate 2-hop or even 3-hop neighbors.  Since usually computing all 2-hop and 3-hop neighbors (i.e. computing the square or the cube of the adjacency matrix) is impractical, sampling neighbors by random walk is a way to approximate.\n",
    "\n",
    "Note that the returned graph will not be a subgraph of the original graph in this case.\n",
    "\n",
    "* The nodes of the returned graph will be the same as the original one, but the edges of the returned graph will instead indicate message passing directions for k-hop neighbors.\n",
    "* You will need to copy the node features from the original graph to the sampled graph yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KHopNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, fanouts, k):\n",
    "        super().__init__(len(fanouts), return_eids=False)\n",
    "        self.k = k\n",
    "        self.fanouts = fanouts\n",
    "        \n",
    "    def sample_frontier(self, layer_id, g, seed_nodes):\n",
    "        #### ATTENTION\n",
    "        # Currently the seed nodes can be either a dict or a tensor.\n",
    "        if isinstance(seed_nodes, dict):\n",
    "            seed_nodes = next(iter(seed_nodes.values()))\n",
    "        fanout = self.fanouts[layer_id]\n",
    "        # Generate the number of random walk traces equal to the fanout.\n",
    "        seed_nodes = seed_nodes.repeat_interleave(fanout)\n",
    "        nodes, _ = dgl.sampling.random_walk(g, seed_nodes, length=self.k)\n",
    "        neighbor_nodes = nodes[:, self.k]\n",
    "        # When a random walk cannot continue because of lacking of successors (e.g. a\n",
    "        # node is isolate, with no edges going out), dgl.sampling.random_walk will\n",
    "        # pad the trace with -1.  Since OGB Products have isolated nodes, we should\n",
    "        # look for the -1 entries and remove them.\n",
    "        mask = (neighbor_nodes != -1)\n",
    "        neighbor_nodes = neighbor_nodes[mask]\n",
    "        seed_nodes = seed_nodes[mask]\n",
    "        # Construct a new graph with only the edges involved in message passing.\n",
    "        sg = dgl.graph((neighbor_nodes, seed_nodes), num_nodes=g.num_nodes())\n",
    "        # Copy the node data from the original graph.\n",
    "        # The edges in the returned graph may not exist in the original graph anyway, so we do not have to\n",
    "        # copy the edge data.\n",
    "        sg.ndata.update(g.ndata)\n",
    "        return sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = KHopNeighborSampler([4, 4], 2)\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "input_nodes, output_nodes, bipartites = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
