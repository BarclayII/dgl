{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Blitz Introduction to DGL - Node Classification (TO BE REFINED)\n",
    "\n",
    "Goal of this tutorial:\n",
    "\n",
    "* Train a node classification neural network on a single small graph.\n",
    "\n",
    "This tutorial assumes that you have experience in building neural networks with PyTorch.  DGL also supports MXNet and Tensorflow whose tutorials are upcoming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Cora Dataset\n",
    "\n",
    "The study of community structure in graphs has a long history. Many proposed methods are *unsupervised* (or *self-supervised* by recent definition), where the model predicts the community labels only by connectivity. Recently, [Kipf et al.,](https://arxiv.org/abs/1609.02907) proposed to formulate the community detection problem as a semi-supervised node classification task. With the help of only a small portion of labeled nodes, a graph neural network (GNN) can accurately predict the community labels of the others.\n",
    "\n",
    "This tutorial will show how to build such a GNN for semi-supervised node classification with only a small number of labels on [Cora dataset](https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.CoraGraphDataset), a citation network with papers as nodes and citations as edges.  The papers contain word count vectorization as features, normalized so that they sum up to 1, as in Section 5.2 in [the paper](https://arxiv.org/abs/1609.02907)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "Number of categories: 7\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "print('Number of categories:', dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [DGL Dataset object](https://docs.dgl.ai/api/python/dgl.data.html) may contain one or multiple graphs.  The Cora dataset used in this tutorial only consists of one single graph.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note: </b>For more details in how DGL organizes its Dataset objects, see <a href=https://docs.dgl.ai/guide/data.html>here</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL graphs can store node-wise and edge-wise information in [`ndata`](https://docs.dgl.ai/generated/dgl.DGLGraph.ndata.html#dgl.DGLGraph.ndata) and [`edata`](https://docs.dgl.ai/generated/dgl.DGLGraph.edata.html#dgl.DGLGraph.edata) attribute as dictionaries.  In the DGL Cora dataset, the graph contains:\n",
    "\n",
    "* `train_mask`: Whether the node is in training set.\n",
    "* `val_mask`: Whether the node is in validation set.\n",
    "* `test_mask`: Whether the node is in test set.\n",
    "* `label`: The ground truth node category.\n",
    "* `feat`: The node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Edge features\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print('Node features')\n",
    "print(g.ndata)\n",
    "print('Edge features')\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <b>Note: </b>For loading data from your own dataset, please refer to <a href=2_load_data.ipynb>this tutorial</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Graph Convolutional Network (GCN)\n",
    "\n",
    "This tutorial will build a two-layer Graph Convolutional Network (GCN).  Each of its layer computes new node representations by aggregating neighbor information.  The equations are:\n",
    "\n",
    "$$\n",
    "h_v^k\\leftarrow \\sum_{u\\in\\mathcal{N}(v)} \\dfrac{1}{c_{uv}} \\mathbf{W}^k h_u^{k-1}\n",
    "$$\n",
    "\n",
    "To build a multi-layer GCN you can simply stack `dgl.nn.GraphConv` modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# Create the model with given dimensions \n",
    "# input layer dimension: 5, node embeddings\n",
    "# hidden layer dimension: 16\n",
    "# output layer dimension: 2, the two classes, 0 and 1\n",
    "net = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL provides implementation of many popular neighbor aggregation modules.  They all can be invoked easily with one line of code.  See the full list of supported [graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GCN\n",
    "\n",
    "Training GCN on the entire graph is no different from training other PyTorch neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.946, val acc: 0.278 (best 0.278), test acc: 0.290 (best 0.290)\n",
      "In epoch 5, loss: 1.888, val acc: 0.596 (best 0.596), test acc: 0.580 (best 0.580)\n",
      "In epoch 10, loss: 1.805, val acc: 0.638 (best 0.638), test acc: 0.658 (best 0.658)\n",
      "In epoch 15, loss: 1.699, val acc: 0.660 (best 0.660), test acc: 0.692 (best 0.692)\n",
      "In epoch 20, loss: 1.572, val acc: 0.688 (best 0.688), test acc: 0.701 (best 0.701)\n",
      "In epoch 25, loss: 1.426, val acc: 0.694 (best 0.694), test acc: 0.710 (best 0.708)\n",
      "In epoch 30, loss: 1.266, val acc: 0.710 (best 0.710), test acc: 0.721 (best 0.721)\n",
      "In epoch 35, loss: 1.100, val acc: 0.714 (best 0.714), test acc: 0.725 (best 0.725)\n",
      "In epoch 40, loss: 0.936, val acc: 0.714 (best 0.716), test acc: 0.729 (best 0.725)\n",
      "In epoch 45, loss: 0.781, val acc: 0.722 (best 0.722), test acc: 0.736 (best 0.736)\n",
      "In epoch 50, loss: 0.641, val acc: 0.742 (best 0.742), test acc: 0.736 (best 0.736)\n",
      "In epoch 55, loss: 0.522, val acc: 0.752 (best 0.752), test acc: 0.741 (best 0.738)\n",
      "In epoch 60, loss: 0.423, val acc: 0.756 (best 0.756), test acc: 0.745 (best 0.745)\n",
      "In epoch 65, loss: 0.343, val acc: 0.754 (best 0.756), test acc: 0.751 (best 0.745)\n",
      "In epoch 70, loss: 0.279, val acc: 0.754 (best 0.756), test acc: 0.758 (best 0.745)\n",
      "In epoch 75, loss: 0.229, val acc: 0.760 (best 0.760), test acc: 0.759 (best 0.761)\n",
      "In epoch 80, loss: 0.189, val acc: 0.764 (best 0.764), test acc: 0.765 (best 0.765)\n",
      "In epoch 85, loss: 0.158, val acc: 0.766 (best 0.766), test acc: 0.766 (best 0.765)\n",
      "In epoch 90, loss: 0.134, val acc: 0.766 (best 0.768), test acc: 0.770 (best 0.767)\n",
      "In epoch 95, loss: 0.114, val acc: 0.768 (best 0.768), test acc: 0.770 (best 0.767)\n"
     ]
    }
   ],
   "source": [
    "def train(g, net):\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "    all_logits = []\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits = net(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that we should only compute the losses of the nodes in the training set,\n",
    "        # i.e. with train_mask 1.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_logits.append(logits.detach())\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "net = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU\n",
    "\n",
    "Training on GPU requires to put both the model and the graph onto GPU with the [`to`](https://docs.dgl.ai/generated/dgl.DGLGraph.to.html#dgl.DGLGraph.to) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.947, val acc: 0.104 (best 0.104), test acc: 0.105 (best 0.105)\n",
      "In epoch 5, loss: 1.913, val acc: 0.530 (best 0.530), test acc: 0.509 (best 0.509)\n",
      "In epoch 10, loss: 1.846, val acc: 0.570 (best 0.628), test acc: 0.567 (best 0.629)\n",
      "In epoch 15, loss: 1.754, val acc: 0.664 (best 0.664), test acc: 0.639 (best 0.639)\n",
      "In epoch 20, loss: 1.633, val acc: 0.700 (best 0.700), test acc: 0.716 (best 0.716)\n",
      "In epoch 25, loss: 1.488, val acc: 0.698 (best 0.702), test acc: 0.703 (best 0.711)\n",
      "In epoch 30, loss: 1.324, val acc: 0.702 (best 0.702), test acc: 0.705 (best 0.711)\n",
      "In epoch 35, loss: 1.150, val acc: 0.714 (best 0.714), test acc: 0.723 (best 0.719)\n",
      "In epoch 40, loss: 0.975, val acc: 0.718 (best 0.718), test acc: 0.736 (best 0.730)\n",
      "In epoch 45, loss: 0.809, val acc: 0.728 (best 0.728), test acc: 0.743 (best 0.743)\n",
      "In epoch 50, loss: 0.661, val acc: 0.736 (best 0.736), test acc: 0.755 (best 0.755)\n",
      "In epoch 55, loss: 0.535, val acc: 0.754 (best 0.754), test acc: 0.761 (best 0.759)\n",
      "In epoch 60, loss: 0.430, val acc: 0.768 (best 0.768), test acc: 0.761 (best 0.761)\n",
      "In epoch 65, loss: 0.346, val acc: 0.772 (best 0.772), test acc: 0.763 (best 0.761)\n",
      "In epoch 70, loss: 0.279, val acc: 0.766 (best 0.772), test acc: 0.768 (best 0.761)\n",
      "In epoch 75, loss: 0.227, val acc: 0.774 (best 0.774), test acc: 0.774 (best 0.774)\n",
      "In epoch 80, loss: 0.186, val acc: 0.774 (best 0.776), test acc: 0.781 (best 0.775)\n",
      "In epoch 85, loss: 0.154, val acc: 0.774 (best 0.776), test acc: 0.782 (best 0.775)\n",
      "In epoch 90, loss: 0.129, val acc: 0.778 (best 0.780), test acc: 0.783 (best 0.782)\n",
      "In epoch 95, loss: 0.109, val acc: 0.778 (best 0.780), test acc: 0.783 (best 0.782)\n"
     ]
    }
   ],
   "source": [
    "g = g.to('cuda')\n",
    "net = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')\n",
    "train(g, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "* [Load and process your own graph data](2_load_data.ipynb).\n",
    "* [Write your own GNN module](3_message_passing.ipynb).\n",
    "* [Link prediction (predicting existence of edges) on full graph](4_link_predict.ipynb).\n",
    "* [Graph classification (TODO)](5_graph_classification.ipynb).\n",
    "* If you wish to scale your model to a large graph, please begin with the tutorial [Stochastic Training of GNN for Node Classification on Large Graphs](L1_large_node_classification.ipynb).\n",
    "* If you have heterogeneous graphs, please begin with the tutorial [Node classification on heterogeneous graphs (TODO)](H1_node_classification.ipynb).\n",
    "* [Categorization of DGL examples](I1_examples.ipynb)\n",
    "* [More tutorials](I2_tutorials.ipynb)\n",
    "\n",
    "### Advanced topics\n",
    "\n",
    "* [Running with multiple GPUs (TODO)]()\n",
    "* [Running with distributed training (TODO)]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
