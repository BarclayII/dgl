{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Blitz Introduction to DGL - Node Classification\n",
    "\n",
    "GNNs are powerful tools for many machine learning tasks on graphs.  In this introductory tutorial, you will learn the basic workflow of using GNNs for node classification, i.e. predicting the category of a node in a graph.\n",
    "\n",
    "Goal of this tutorial:\n",
    "\n",
    "* Train a node classification neural network on a single small graph.\n",
    "\n",
    "This tutorial assumes that you have experience in building neural networks with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Node Classification with GNN\n",
    "\n",
    "Many proposed methods are *unsupervised* (or *self-supervised* by recent definition), where the model predicts the community labels only by connectivity. Recently, [Kipf et al.,](https://arxiv.org/abs/1609.02907) proposed to formulate the community detection problem as a node classification task. With the help of only a small portion of labeled nodes, a graph neural network (GNN) can accurately predict the community labels of the others.\n",
    "\n",
    "This tutorial will show how to build such a GNN for semi-supervised node classification with only a small number of labels on [Cora dataset](https://docs.dgl.ai/api/python/dgl.data.html#dgl.data.CoraGraphDataset), a citation network with papers as nodes and citations as edges.  The task is to predict the category of a given paper.  The papers contain word count vectorization as features, normalized so that they sum up to 1, as in Section 5.2 in [the paper](https://arxiv.org/abs/1609.02907).\n",
    "\n",
    "## Loading Cora Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n",
      "Number of categories: 7\n"
     ]
    }
   ],
   "source": [
    "import dgl.data\n",
    "\n",
    "dataset = dgl.data.CoraGraphDataset()\n",
    "print('Number of categories:', dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A DGL Dataset object may contain one or multiple graphs.  The Cora dataset used in this tutorial only consists of one single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL graphs can store node-wise and edge-wise information in `ndata` and `edata` attribute as dictionaries.  In the DGL Cora dataset, the graph contains:\n",
    "\n",
    "* `train_mask`: Whether the node is in training set.\n",
    "* `val_mask`: Whether the node is in validation set.\n",
    "* `test_mask`: Whether the node is in test set.\n",
    "* `label`: The ground truth node category.\n",
    "* `feat`: The node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features\n",
      "{'train_mask': tensor([ True,  True,  True,  ..., False, False, False]), 'val_mask': tensor([False, False, False,  ..., False, False, False]), 'test_mask': tensor([False, False, False,  ...,  True,  True,  True]), 'label': tensor([3, 4, 4,  ..., 3, 3, 3]), 'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])}\n",
      "Edge features\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "print('Node features')\n",
    "print(g.ndata)\n",
    "print('Edge features')\n",
    "print(g.edata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Graph Convolutional Network (GCN)\n",
    "\n",
    "This tutorial will build a two-layer [Graph Convolutional Network (GCN)](http://tkipf.github.io/graph-convolutional-networks/).  Each of its layer computes new node representations by aggregating neighbor information.\n",
    "\n",
    "To build a multi-layer GCN you can simply stack `dgl.nn.GraphConv` modules, which inherit `torch.nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# Create the model with given dimensions\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGL provides implementation of many popular neighbor aggregation modules.  They all can be invoked easily with one line of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the GCN\n",
    "\n",
    "Training this GCN is similar to training other PyTorch neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.946, val acc: 0.132 (best 0.132), test acc: 0.130 (best 0.130)\n",
      "In epoch 5, loss: 1.892, val acc: 0.516 (best 0.540), test acc: 0.546 (best 0.556)\n",
      "In epoch 10, loss: 1.811, val acc: 0.684 (best 0.684), test acc: 0.705 (best 0.702)\n",
      "In epoch 15, loss: 1.705, val acc: 0.724 (best 0.724), test acc: 0.723 (best 0.723)\n",
      "In epoch 20, loss: 1.574, val acc: 0.732 (best 0.740), test acc: 0.741 (best 0.734)\n",
      "In epoch 25, loss: 1.421, val acc: 0.738 (best 0.740), test acc: 0.739 (best 0.734)\n",
      "In epoch 30, loss: 1.251, val acc: 0.744 (best 0.746), test acc: 0.743 (best 0.735)\n",
      "In epoch 35, loss: 1.074, val acc: 0.756 (best 0.756), test acc: 0.745 (best 0.745)\n",
      "In epoch 40, loss: 0.900, val acc: 0.752 (best 0.756), test acc: 0.746 (best 0.745)\n",
      "In epoch 45, loss: 0.739, val acc: 0.754 (best 0.756), test acc: 0.746 (best 0.745)\n",
      "In epoch 50, loss: 0.599, val acc: 0.760 (best 0.760), test acc: 0.748 (best 0.748)\n",
      "In epoch 55, loss: 0.481, val acc: 0.764 (best 0.764), test acc: 0.751 (best 0.750)\n",
      "In epoch 60, loss: 0.386, val acc: 0.766 (best 0.766), test acc: 0.756 (best 0.753)\n",
      "In epoch 65, loss: 0.311, val acc: 0.770 (best 0.774), test acc: 0.760 (best 0.759)\n",
      "In epoch 70, loss: 0.252, val acc: 0.768 (best 0.774), test acc: 0.761 (best 0.759)\n",
      "In epoch 75, loss: 0.205, val acc: 0.770 (best 0.774), test acc: 0.765 (best 0.759)\n",
      "In epoch 80, loss: 0.170, val acc: 0.772 (best 0.774), test acc: 0.761 (best 0.759)\n",
      "In epoch 85, loss: 0.141, val acc: 0.772 (best 0.774), test acc: 0.762 (best 0.759)\n",
      "In epoch 90, loss: 0.119, val acc: 0.772 (best 0.774), test acc: 0.760 (best 0.759)\n",
      "In epoch 95, loss: 0.102, val acc: 0.768 (best 0.774), test acc: 0.758 (best 0.759)\n"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    train_mask = g.ndata['train_mask']\n",
    "    val_mask = g.ndata['val_mask']\n",
    "    test_mask = g.ndata['test_mask']\n",
    "    for e in range(100):\n",
    "        # Forward\n",
    "        logits = model(g, features)\n",
    "\n",
    "        # Compute prediction\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that we should only compute the losses of the nodes in the training set,\n",
    "        # i.e. with train_mask 1.\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes)\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on GPU\n",
    "\n",
    "Training on GPU requires to put both the model and the graph onto GPU with the `to` method, similar to what you will do in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 1.946, val acc: 0.252 (best 0.252), test acc: 0.224 (best 0.224)\n",
      "In epoch 5, loss: 1.896, val acc: 0.432 (best 0.452), test acc: 0.476 (best 0.472)\n",
      "In epoch 10, loss: 1.821, val acc: 0.484 (best 0.484), test acc: 0.503 (best 0.503)\n",
      "In epoch 15, loss: 1.723, val acc: 0.558 (best 0.558), test acc: 0.572 (best 0.572)\n",
      "In epoch 20, loss: 1.605, val acc: 0.622 (best 0.622), test acc: 0.635 (best 0.635)\n",
      "In epoch 25, loss: 1.468, val acc: 0.656 (best 0.656), test acc: 0.658 (best 0.658)\n",
      "In epoch 30, loss: 1.318, val acc: 0.676 (best 0.676), test acc: 0.683 (best 0.681)\n",
      "In epoch 35, loss: 1.161, val acc: 0.696 (best 0.696), test acc: 0.703 (best 0.703)\n",
      "In epoch 40, loss: 1.004, val acc: 0.712 (best 0.712), test acc: 0.711 (best 0.711)\n",
      "In epoch 45, loss: 0.853, val acc: 0.722 (best 0.722), test acc: 0.727 (best 0.727)\n",
      "In epoch 50, loss: 0.715, val acc: 0.736 (best 0.736), test acc: 0.739 (best 0.739)\n",
      "In epoch 55, loss: 0.594, val acc: 0.752 (best 0.752), test acc: 0.744 (best 0.743)\n",
      "In epoch 60, loss: 0.490, val acc: 0.760 (best 0.760), test acc: 0.747 (best 0.747)\n",
      "In epoch 65, loss: 0.404, val acc: 0.766 (best 0.766), test acc: 0.752 (best 0.748)\n",
      "In epoch 70, loss: 0.334, val acc: 0.774 (best 0.774), test acc: 0.753 (best 0.753)\n",
      "In epoch 75, loss: 0.277, val acc: 0.774 (best 0.774), test acc: 0.752 (best 0.753)\n",
      "In epoch 80, loss: 0.231, val acc: 0.774 (best 0.774), test acc: 0.754 (best 0.753)\n",
      "In epoch 85, loss: 0.194, val acc: 0.774 (best 0.776), test acc: 0.757 (best 0.756)\n",
      "In epoch 90, loss: 0.165, val acc: 0.774 (best 0.776), test acc: 0.754 (best 0.756)\n",
      "In epoch 95, loss: 0.141, val acc: 0.774 (best 0.776), test acc: 0.753 (best 0.756)\n"
     ]
    }
   ],
   "source": [
    "g = g.to('cuda')\n",
    "model = GCN(g.ndata['feat'].shape[1], 16, dataset.num_classes).to('cuda')\n",
    "train(g, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "* [How does DGL represent a graph](2_dglgraph.ipynb)?\n",
    "* [Write your own GNN module](3_message_passing.ipynb).\n",
    "* [Link prediction (predicting existence of edges) on full graph](4_link_predict.ipynb).\n",
    "* [Graph classification](5_graph_classification.ipynb).\n",
    "* [Make your own dataset](6_load_data.ipynb).\n",
    "* [The list of supported graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv).\n",
    "\n",
    "### Scale to large graphs\n",
    "* If you wish to scale your model to a large graph, please begin with the tutorial [Stochastic Training of GNN for Node Classification on Large Graphs](L1_large_node_classification.ipynb).\n",
    "\n",
    "### Advanced topics\n",
    "\n",
    "* [Running with multiple GPUs (TODO)]()\n",
    "  * [Large graph]()\n",
    "  * [Graph classification]()\n",
    "* [Running with distributed training (TODO)]()\n",
    "\n",
    "<!--\n",
    "### Other materials\n",
    "* [DGL examples](I1_examples.ipynb): this is a catalog of all end-to-end examples provided by DGL.\n",
    "* [HOWTOs](I2_tutorials.ipynb): this is a catalog of individual articles describing how to build models with DGL for various data and scenarios.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
