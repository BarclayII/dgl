{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Node Classification on Large Graphs\n",
    "\n",
    "This tutorial shows how to train a multi-layer GraphSAGE for node classification on Amazon Copurchase Network provided by [Open Graph Benchmark (OGB)](https://ogb.stanford.edu/).  The dataset contains 2.4 million nodes and 61 million edges, hence not fitting in a single GPU.\n",
    "\n",
    "This tutorial's contents include\n",
    "\n",
    "* Training a GNN model with a single machine, a single GPU, on a graph of any size, with DGL's GNN modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "OGB already prepared the data as DGL graph.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: If you wish to load your own large graph and a single machine's CPU memory can hold it, please refer to <a href=2_load_data.ipynb>this tutorial</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OGB dataset is a collection of graphs and their labels.  The Amazon Copurchase Network dataset only contains a single graph.  So you can simply get the graph and its node labels like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=2449029, num_edges=123718280,\n",
      "      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        ...,\n",
      "        [8],\n",
      "        [2],\n",
      "        [4]])\n",
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "graph, node_labels = dataset[0]\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "node_labels = node_labels[:, 0]\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Question**: you should call `g.formats()` to create all CSR/CSC so that multiprocessing dataloaders will not compute their own CSR/CSC representations of the graph to waste memory.  How should I present it?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the training-validation-test split of the nodes with `get_split_idx` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Message Passing in Detail\n",
    "\n",
    "The message passing formulation defined in [Gilmer et al.](https://arxiv.org/abs/1704.01212) goes as below:\n",
    "\n",
    "$$\n",
    "m_{u\\to v}^{(l)} = M^{(l)}\\left(h_v^{(l-1)}, h_u^{(l-1)}, e_{u\\to v}^{(l-1)}\\right) \\\\\n",
    "m_{v}^{(l)} = \\sum_{u\\in\\mathcal{N}(v)}m_{u\\to v}^{(l)} \\\\\n",
    "h_v^{(l)} = U^{(l)}\\left(h_v^{(l-1)}, m_v^{(l)}\\right)\n",
    "$$\n",
    "\n",
    "Essentially, the $l$-th layer representation of a node depends on the $(l-1)$-th layer representation of the same node, as well as the $(l-1)$-th layer representation of the neighboring nodes.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note: </b>See <a href=3_message_passing.ipynb>this tutorial</a> for more details in message passing in DGL.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you would like to compute the representation of the red node by a 2-layer GNN:\n",
    "\n",
    "![Imgur](assets/seed.png)\n",
    "\n",
    "The formulation shows that to compute the red node's second GNN layer output $\\boldsymbol{h}_8^{(2)}$ you will need the first GNN layer output of the same node $\\boldsymbol{h}_8^{(1)}$, as well as the first GNN layer output of its neighboring nodes $\\boldsymbol{h}_4^{(1)}$, $\\boldsymbol{h}_5^{(1)}$, $\\boldsymbol{h}_7^{(1)}$, and $\\boldsymbol{h}_{11}^{(1)}$ (colored green).  The message passing will happen on the green dashed edges visualized below.\n",
    "\n",
    "![Imgur](assets/3.png)\n",
    "\n",
    "To compute the first-layer representation of the red and green nodes, you further need to perform message passing on the yellow edges visualized below.  Therefore, other than the red and green nodes, the yellow nodes' input features are also necessary to compute the red nodes' second GNN layer output.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note</b>: The edges that showed up in the second layer (i.e. as the green dashed arrows) appears again in the first layer (i.e. as the yellow dashed arrows), but they represent different message passing computations.  The second layer computes from $\\boldsymbol{h}_\\cdot^{(1)}$ to $\\boldsymbol{h}_\\cdot^{(2)}$ while the first layer computes from $\\boldsymbol{h}_\\cdot^{(0)}$ (i.e. the input) to $\\boldsymbol{h}_\\cdot^{(1)}$.\n",
    "</div>\n",
    "\n",
    "![Imgur](assets/4.png)\n",
    "\n",
    "You may notice that to figure out which nodes' input features are necessary, you are going in the opposite direction of message aggregation: you start from the layer closest to the output and work backward to the input.  Message passing, in contrast, goes from the layer closest to the input towards the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbor Sampling Overview\n",
    "\n",
    "You can also see from the previous example that computing representation for a small number of nodes often requires input features of a significantly larger number of nodes.  Taking all neighbors for message aggregation is often too costly since the nodes needed for input features would easily cover a large portion of the graph, especially for real-world graphs which are often [scale-free](https://en.wikipedia.org/wiki/Scale-free_network).\n",
    "\n",
    "Neighbor sampling addresses this issue by selecting a subset of the neighbors to perform aggregation.  For instance, to compute $\\boldsymbol{h}_8^{(1)}$, you can choose two of the neighbors instead of all of them to aggregate, so you need the first layer representation of the red node and only two green nodes.\n",
    "\n",
    "![Imgur](assets/5.png)\n",
    "\n",
    "Similarly, to compute the red and green nodes' first layer representation, you can also do neighbor sampling that takes two of each node's neighbors.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <b>Note</b>: Since you need the first layer representation of the red node, you will need to sample the neighbors of the red node again.\n",
    "</div>\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "You can see that this method could give us fewer nodes needed for input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining neighbor sampler and data loader in DGL\n",
    "\n",
    "DGL provides useful tools to generate such computation dependencies while iterating over the dataset in minibatches.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset, and `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with neighbor sampling.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 2-layer GraphSAGE with neighbor sampling, and each node will gather message from 4 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over the data loader and see what it yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([129839, 123246, 162633,  ...,  58772, 793634, 151106]), tensor([129839, 123246, 162633,  ..., 140799, 167173, 185392]), [Block(num_src_nodes=23628, num_dst_nodes=5061, num_edges=20200), Block(num_src_nodes=5061, num_dst_nodes=1024, num_edges=4088)])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 nodes' outputs, we need 23628 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} nodes' outputs, we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `bipartites` shows how messages aggregate on each layer.  As its name suggests, it is a **list** of bipartite graphs.\n",
    "\n",
    "So why does DGL return a list of *bipartite* graphs for training a *homogeneous* graph?  The reason is that the number of nodes for input and that for output of a given GNN layer is different.  Take the example above:\n",
    "\n",
    "![Imgur](assets/6.png)\n",
    "\n",
    "That GNN layer will output the representation of three nodes (two green nodes and one red node), but it will require input from 7 nodes (the green nodes and red node, plus 4 yellow nodes).  Only a bipartite graph can describe such computation:\n",
    "\n",
    "![](assets/bipartite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch training of GNNs usually involves message passing on such bipartite graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Block(num_src_nodes=23628, num_dst_nodes=5061, num_edges=20200), Block(num_src_nodes=5061, num_dst_nodes=1024, num_edges=4088)]\n"
     ]
    }
   ],
   "source": [
    "print(bipartites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        h = self.conv1(bipartites[0], x)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(bipartites[1], h)\n",
    "        return h\n",
    "    \n",
    "model = Model(num_features, 128, num_classes).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare against the code in the [introduction](1_introduction.ipynb), you will notice a difference in `forward()` function where instead of computing on the full graph:\n",
    "\n",
    "```python\n",
    "h = self.conv1(g, x)\n",
    "```\n",
    "\n",
    "you only compute on the sampled bipartite graph:\n",
    "\n",
    "```python\n",
    "h = self.conv1(bipartites[0], x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 35.30it/s, loss=0.512, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 46.53it/s]\n",
      "  2%|▏         | 4/193 [00:00<00:05, 37.67it/s, loss=0.894, acc=0.785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.793988251150726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 46.67it/s, loss=1.074, acc=0.571]\n",
      "100%|██████████| 39/39 [00:00<00:00, 65.66it/s]\n",
      "  3%|▎         | 5/193 [00:00<00:04, 45.37it/s, loss=0.643, acc=0.844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.8278106960303131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 40.74it/s, loss=0.656, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 53.47it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:06, 28.02it/s, loss=0.565, acc=0.847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.8419754342242454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 41.73it/s, loss=0.410, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 54.97it/s]\n",
      "  3%|▎         | 5/193 [00:00<00:04, 46.03it/s, loss=0.588, acc=0.841]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.8506980647458231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 41.32it/s, loss=0.057, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 58.68it/s]\n",
      "  3%|▎         | 5/193 [00:00<00:04, 42.71it/s, loss=0.463, acc=0.862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.8559112987310226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 41.21it/s, loss=0.144, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 48.56it/s]\n",
      "  2%|▏         | 4/193 [00:00<00:05, 34.52it/s, loss=0.486, acc=0.875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.8609973806678025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 46.25it/s, loss=0.102, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 51.76it/s]\n",
      "  1%|          | 2/193 [00:00<00:13, 14.63it/s, loss=0.427, acc=0.879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.8643287643363935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 32.19it/s, loss=0.055, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 47.01it/s]\n",
      "  2%|▏         | 4/193 [00:00<00:05, 32.68it/s, loss=0.372, acc=0.907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.8668718053047835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:06<00:00, 31.50it/s, loss=0.027, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 26.57it/s]\n",
      "  1%|          | 2/193 [00:00<00:12, 15.18it/s, loss=0.424, acc=0.881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.8682196170180302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 35.87it/s, loss=0.189, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 43.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.8715255702769371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels.append(node_labels[output_nodes].numpy())\n",
    "            predictions.append(model(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Inference without Neighbor Sampling\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Question**: The same content appeared in the user guide.  Should I keep it in the user guide or move it to the tutorial?\n",
    "\n",
    "</div>\n",
    "\n",
    "Usually for offline inference it is desirable to aggregate over the entire neighborhood to eliminate randomness introduced by neighbor sampling.  However, using the same methodology in training is not efficient, because there will be a lot of redundant computation.  Moreover, simply doing neighbor sampling by taking all neighbors will often exhaust GPU memory because the number of nodes required for input features may be too large to fit into GPU memory.|\n",
    "\n",
    "Instead, you need to compute the representations layer by layer: you first compute the output of the first GNN layer for all nodes, then you compute the output of second GNN layer for all nodes using the first GNN layer's output as input, etc.  This gives us a different algorithm from what is being used in training.  During training you will have an outer loop that iterates over the nodes, and an inner loop that iterates over the layers.  In contrast, during inference you will have an outer loop that iterates over the layers, and an inner loop that iterates over the nodes.\n",
    "\n",
    "If you do not care about randomness too much (e.g., during model selection in validation), you can still use the `dgl.dataloading.MultiLayerNeighborSampler` and `dgl.dataloading.NodeDataLoader` to do offline inference, since it is usually faster for evaluating a small number of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](assets/anim.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    layers = [model.conv1, model.conv2]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = torch.zeros(\n",
    "                graph.number_of_nodes(), model.h_feats if l != len(layers) - 1 else dataset.num_classes)\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != len(layers) - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads the best model from the file saved previously and performs offline inference.  It computes the accuracy on the test set afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 299/299 [00:58<00:00,  5.14it/s]\n",
      "100%|██████████| 299/299 [01:01<00:00,  4.88it/s]\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(best_model_path))\n",
    "all_predictions = inference(model, graph, node_features, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7402727678165968\n"
     ]
    }
   ],
   "source": [
    "test_predictions = all_predictions[test_nids].argmax(1)\n",
    "test_labels = node_labels[test_nids]\n",
    "test_accuracy = sklearn.metrics.accuracy_score(test_predictions.numpy(), test_labels.numpy())\n",
    "print('Test accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE with neighbor sampling on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "* [Stochastic Training of GNN for Link Prediction on Large Graphs](L2_large_link_prediction.ipynb).\n",
    "* [Customizing Neighbor Sampler (TODO)]().\n",
    "* [Adapting your custom GNN module for large scale training (TODO)]().\n",
    "\n",
    "For large-scale heterogeneous graph training, please see [Scaling to large heterogeneous graphs](H5_large_heterogeneous_graph.ipynb).  We recommend you going through the tutorial [Node Classification on Heterogeneous Graphs](H1_node_classification.ipynb) to grasp an idea of how DGL handles heterogeneous graphs first.\n",
    "\n",
    "For single-machine multi-GPU training on a single large graph, please see the tutorial [Stochastic Training of GNN with Multiple GPUs](D2_multi_gpu_large_graph.ipynb).\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Question**: should I talk about how to train node classification on a weighted graph (e.g. training a GCN is equivalent to training on a weighted graph)?  It will involve non-uniform neighbor sampling (which DGL has a not-efficient-enough support).  I thought of how to train a GCN with neighbor sampling, and it turns out that I should write a GIN with non-uniform neighbor sampling with replacement and aggregator type `sum`.\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
