{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Node Classification\n",
    "\n",
    "This tutorial shows how to train a multi-layer GraphSAGE for node classification on Amazon Copurchase Network provided by [Open Graph Benchmark (OGB)](https://ogb.stanford.edu/).  The dataset contains 2.4 million nodes and 61 million edges, hence not fitting in a single GPU.\n",
    "\n",
    "This tutorial's contents include\n",
    "\n",
    "* Training a GNN model with a single machine, a single GPU, on a graph of any size, with DGL's GNN modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "OGB already prepared the data as DGL graph.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: If you wish to load your own large graph and a single machine's CPU memory can hold it, please refer to <a href=2_load_data.ipynb>this tutorial</a>.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OGB dataset is a collection of graphs and their labels.  The Amazon Copurchase Network dataset only contains a single graph.  So you can simply get the graph and its node labels like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=2449029, num_edges=123718280,\n",
      "      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        ...,\n",
      "        [8],\n",
      "        [2],\n",
      "        [4]])\n",
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "graph, node_labels = dataset[0]\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "node_labels = node_labels[:, 0]\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Question**: you should call `g.formats()` to create all CSR/CSC so that multiprocessing dataloaders will not compute their own CSR/CSC representations of the graph to waste memory.  How should I present it?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the training-validation-test split of the nodes with `get_split_idx` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neighbor Sampler and Data Loader in DGL\n",
    "\n",
    "DGL provides useful tools to iterate over the dataset in minibatches while generating the computation dependencies to compute their outputs without involving all the nodes.  For node classification, you can use `dgl.dataloading.NodeDataLoader` for iterating over the dataset.  Then you can use `dgl.dataloading.MultiLayerNeighborSampler` to generate computation dependencies of the nodes from a multi-layer GNN with *neighbor sampling*, i.e. taking only a fixed number of neighbors for each node to aggregate messages.\n",
    "\n",
    "The syntax of `dgl.dataloading.NodeDataLoader` is mostly similar to a PyTorch `DataLoader`, with the addition that it needs a graph to generate computation dependency from, a set of node IDs to iterate on, and the neighbor sampler you defined.\n",
    "\n",
    "Let's consider training a 2-layer GraphSAGE with neighbor sampling, and each node will gather message from 4 neighbors on each layer.  The code defining the data loader and neighbor sampler will look like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can iterate over the data loader and see what it yields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([129839, 123246, 162633,  ...,  58772, 793634, 151106]), tensor([129839, 123246, 162633,  ..., 140799, 167173, 185392]), [Block(num_src_nodes=23628, num_dst_nodes=5061, num_edges=20200), Block(num_src_nodes=5061, num_dst_nodes=1024, num_edges=4088)])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NodeDataLoader` gives us three items per iteration.\n",
    "\n",
    "* The input node list for the nodes whose input features are needed to compute the outputs.\n",
    "* The output node list whose GNN representation are to be computed.\n",
    "* The list of computation dependency for each layer as a list of **bipartite graphs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To compute 1024 nodes' outputs, we need 23628 nodes' input features\n"
     ]
    }
   ],
   "source": [
    "input_nodes, output_nodes, bipartites = example_minibatch\n",
    "print(\"To compute {} nodes' outputs, we need {} nodes' input features\".format(len(output_nodes), len(input_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Block(num_src_nodes=23628, num_dst_nodes=5061, num_edges=20200), Block(num_src_nodes=5061, num_dst_nodes=1024, num_edges=4088)]\n"
     ]
    }
   ],
   "source": [
    "print(bipartites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minibatch training of GNNs usually involves message passing on such bipartite graphs.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "**Note**: if you are interested in the details of neighbor sampling, or if you are curious about why neighbor sampling will yield a list of *bipartite* graphs instead of simply some subgraphs of the original graph, please refer to the [neighbor sampling tutorial](L3_custom_sampler.ipynb).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model\n",
    "\n",
    "The model can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        h = self.conv1(bipartites[0], x)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(bipartites[1], h)\n",
    "        return h\n",
    "    \n",
    "model = Model(num_features, 128, num_classes).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you compare against the code in the [introduction](1_introduction.ipynb), you will notice a difference in `forward()` function where instead of computing on the full graph:\n",
    "\n",
    "```python\n",
    "h = self.conv1(g, x)\n",
    "```\n",
    "\n",
    "you only compute on the sampled bipartite graph:\n",
    "\n",
    "```python\n",
    "h = self.conv1(bipartites[0], x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing the validation score for model selection, usually you can also do neighbor sampling.  To do that, you need to define another data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, valid_nids, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a training loop that performs validation every epoch.  It also saves the model with the best validation accuracy into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 35.30it/s, loss=0.512, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 46.53it/s]\n",
      "  2%|▏         | 4/193 [00:00<00:05, 37.67it/s, loss=0.894, acc=0.785]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.793988251150726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 46.67it/s, loss=1.074, acc=0.571]\n",
      "100%|██████████| 39/39 [00:00<00:00, 65.66it/s]\n",
      "  3%|▎         | 5/193 [00:00<00:04, 45.37it/s, loss=0.643, acc=0.844]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation Accuracy 0.8278106960303131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 40.74it/s, loss=0.656, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 53.47it/s]\n",
      "  2%|▏         | 3/193 [00:00<00:06, 28.02it/s, loss=0.565, acc=0.847]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Validation Accuracy 0.8419754342242454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 41.73it/s, loss=0.410, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 54.97it/s]\n",
      "  3%|▎         | 5/193 [00:00<00:04, 46.03it/s, loss=0.588, acc=0.841]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Validation Accuracy 0.8506980647458231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 41.32it/s, loss=0.057, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 58.68it/s]\n",
      "  3%|▎         | 5/193 [00:00<00:04, 42.71it/s, loss=0.463, acc=0.862]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Validation Accuracy 0.8559112987310226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 41.21it/s, loss=0.144, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 48.56it/s]\n",
      "  2%|▏         | 4/193 [00:00<00:05, 34.52it/s, loss=0.486, acc=0.875]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Validation Accuracy 0.8609973806678025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:04<00:00, 46.25it/s, loss=0.102, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 51.76it/s]\n",
      "  1%|          | 2/193 [00:00<00:13, 14.63it/s, loss=0.427, acc=0.879]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Validation Accuracy 0.8643287643363935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 32.19it/s, loss=0.055, acc=1.000]\n",
      "100%|██████████| 39/39 [00:00<00:00, 47.01it/s]\n",
      "  2%|▏         | 4/193 [00:00<00:05, 32.68it/s, loss=0.372, acc=0.907]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Validation Accuracy 0.8668718053047835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:06<00:00, 31.50it/s, loss=0.027, acc=1.000]\n",
      "100%|██████████| 39/39 [00:01<00:00, 26.57it/s]\n",
      "  1%|          | 2/193 [00:00<00:12, 15.18it/s, loss=0.424, acc=0.881]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Validation Accuracy 0.8682196170180302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193/193 [00:05<00:00, 35.87it/s, loss=0.189, acc=0.857]\n",
      "100%|██████████| 39/39 [00:00<00:00, 43.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Validation Accuracy 0.8715255702769371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    labels = []\n",
    "    with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
    "        for input_nodes, output_nodes, bipartites in tq:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels.append(node_labels[output_nodes].numpy())\n",
    "            predictions.append(model(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "        predictions = np.concatenate(predictions)\n",
    "        labels = np.concatenate(labels)\n",
    "        accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "        print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "        if best_accuracy < accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE with neighbor sampling on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU.\n",
    "\n",
    "## What's next?\n",
    "\n",
    "* [Stochastic training of GNN for link prediction](L2_large_link_prediction.ipynb).\n",
    "* [Neighbor sampling and customization](L3_custom_sampler.ipynb).\n",
    "* [Adapting your custom GNN module for stochastic training](L4_message_passing.ipynb).\n",
    "* During inference you may wish to disable neighbor sampling.  If so, please refer to the [user guide on exact offline inference](https://docs.dgl.ai/guide/minibatch-inference.html).\n",
    "\n",
    "For large-scale heterogeneous graph training, please see [Scaling to large heterogeneous graphs](H5_large_heterogeneous_graph.ipynb).  We recommend you going through the tutorial [Node Classification on Heterogeneous Graphs](H1_node_classification.ipynb) to grasp an idea of how DGL handles heterogeneous graphs first.\n",
    "\n",
    "For single-machine multi-GPU training on a single large graph, please see the tutorial [Stochastic Training of GNN with Multiple GPUs](D2_multi_gpu_large_graph.ipynb).\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "**Question**: should I talk about how to train node classification on a weighted graph (e.g. training a GCN is equivalent to training on a weighted graph)?  It will involve non-uniform neighbor sampling (which DGL has a not-efficient-enough support).  I thought of how to train a GCN with neighbor sampling, and it turns out that I should write a GIN with non-uniform neighbor sampling with replacement and aggregator type `sum`.\n",
    "     \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bipartites' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-929f4740ee37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbipartites\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bipartites' is not defined"
     ]
    }
   ],
   "source": [
    "bipartites[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
