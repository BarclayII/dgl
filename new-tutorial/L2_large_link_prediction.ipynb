{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Training of GNN for Link Prediction \n",
    "\n",
    "This tutorial will show how to train a multi-layer GraphSAGE for link prediction on Amazon Copurchase Network provided by OGB.  The dataset contains 2.4 million nodes and 61 million edges, hence not fitting a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction Overview\n",
    "\n",
    "Link prediction requires the model to predict the probability of existence of an edge.  This tutorial does so by computing a dot product between the representations of both incident nodes.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{u\\sim v} = \\sigma(h_u^T h_v)\n",
    "$$\n",
    "\n",
    "It then minimizes the following binary cross entropy loss.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\n",
    "$$\n",
    "\n",
    "This is identical to the link prediction formulation in [the previous tutorial on link prediction](4_link_predict.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "This tutorial loads the dataset from the `ogb` package as in the [previous tutorial](L1_large_node_classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n",
      "WARNING:root:The OGB package is out of date. Your version is 1.2.3, while the latest version is 1.2.4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=2449029, num_edges=123718280,\n",
      "      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float32)}\n",
      "      edata_schemes={})\n",
      "tensor([[0],\n",
      "        [1],\n",
      "        [2],\n",
      "        ...,\n",
      "        [8],\n",
      "        [2],\n",
      "        [4]])\n",
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "\n",
    "dataset = DglNodePropPredDataset('ogbn-products')\n",
    "\n",
    "graph, node_labels = dataset[0]\n",
    "print(graph)\n",
    "print(node_labels)\n",
    "\n",
    "node_features = graph.ndata['feat']\n",
    "node_labels = node_labels[:, 0]\n",
    "num_features = node_features.shape[1]\n",
    "num_classes = (node_labels.max() + 1).item()\n",
    "print('Number of classes:', num_classes)\n",
    "\n",
    "idx_split = dataset.get_idx_split()\n",
    "train_nids = idx_split['train']\n",
    "valid_nids = idx_split['valid']\n",
    "test_nids = idx_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Neighbor Sampler and Data Loader in DGL\n",
    "\n",
    "Different from the [link prediction tutorial for small graph](4_link_predict.ipynb), you will need to iterate over the edges in minibatches, since computing the probability of all edges is usually impossible.  For each minibatch of edges, you compute the output representation of their incident nodes using neighbor sampling and GNN, in a similar fashion introduced in the [large-scale node classification tutorial](L1_large_node_classification.ipynb).\n",
    "\n",
    "DGL provides `dgl.dataloading.EdgeDataLoader` that allows you to iterate over edges for edge classification or link prediction tasks.\n",
    "\n",
    "To perform link prediction, you need to specify a negative sampler.  A negative sampler takes in a list of edges as positive examples and returns a list of negative examples.  In DGL, negative samplers can be any callable that has the following signature:\n",
    "\n",
    "```python\n",
    "def negative_sampler(g: DGLGraph, eids: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    pass\n",
    "```\n",
    "\n",
    "The first argument is the original graph and the second argument is the minibatch of edge IDs.  The function returns a pair of $u$-$v^-$ node ID tensors as negative examples.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: for heterogeneous graphs, the signature of the negative sampler will change.  See [here](https://todo) for more details.\n",
    "    \n",
    "</div>\n",
    "\n",
    "The following code implements a negative sampler that find non-existent edges by sampling `k` $v^-$ for each $u$ according to a distribution $P^-(v) \\propto d(v)^{0.75}$, where $d(v)$ is the degree of $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k):\n",
    "        self.k = k\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        dst = self.weights.multinomial(len(src), replacement=True)\n",
    "        return src, dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the negative sampler, one can then define the edge data loader with neighbor sampling.  Here this tutorial takes 5 negative examples per positive example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "k = 5\n",
    "train_dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "    graph, torch.arange(graph.number_of_edges()), sampler,\n",
    "    negative_sampler=NegativeSampler(graph, k),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can peek one minibatch from `train_dataloader` and see what it will give you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([2223486, 1488848, 1690274,  ..., 1601512,  288853,  258641]), Graph(num_nodes=7142, num_edges=1024,\n",
      "      ndata_schemes={'feat': Scheme(shape=(100,), dtype=torch.float32), '_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=7142, num_edges=5120,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), [Block(num_src_nodes=146105, num_dst_nodes=34484, num_edges=137581), Block(num_src_nodes=34484, num_dst_nodes=7142, num_edges=28455)])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example minibatch consists of four elements.\n",
    "\n",
    "* The input node list necessary for computing the representation of output nodes.\n",
    "* The subgraph induced by the nodes being sampled in the minibatch (including those in the negative examples) as well as the edges sampled in the minibatch.\n",
    "* The subgraph induced by the nodes being sampled in the minibatch (including those in the negative examples) as well as the non-existent edges sampled by the negative sampler.\n",
    "* The list of bipartite graphs, one for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input nodes: 146105\n",
      "Positive graph # nodes: 7142 # edges: 1024\n",
      "Negative graph # noeds: 7142 # edges: 5120\n",
      "[Block(num_src_nodes=146105, num_dst_nodes=34484, num_edges=137581), Block(num_src_nodes=34484, num_dst_nodes=7142, num_edges=28455)]\n"
     ]
    }
   ],
   "source": [
    "input_nodes, pos_graph, neg_graph, bipartites = example_minibatch\n",
    "print('Number of input nodes:', len(input_nodes))\n",
    "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
    "print('Negative graph # noeds:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\n",
    "print(bipartites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model for Node Representation\n",
    "\n",
    "The model is almost identical to the one in the [node classification tutorial](L1_large_node_classification.ipynb).  The only difference is that since you are doing link prediction, the output dimension will not be the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn import SAGEConv\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, aggregator_type='mean')\n",
    "        self.h_feats = h_feats\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        h = self.conv1(bipartites[0], x)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(bipartites[1], h)\n",
    "        return h\n",
    "    \n",
    "model = Model(num_features, 128).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Score Predictor for Edges\n",
    "\n",
    "After getting the node representation necessary for the minibatch, the last thing to do is to predict the score of the edges and non-existent edges in the sampled minibatch.  This can be easily accomplished with `apply_edges` method.  Here, this tutorial will simply compute the score by dot product of the representations of both incident nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, subgraph, x):\n",
    "        with subgraph.local_scope():\n",
    "            subgraph.ndata['x'] = x\n",
    "            subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'))\n",
    "            return subgraph.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "There are various ways to evaluate the performance of link prediction.  This tutorial follows the practice of [GraphSAGE paper](https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf), where it treats the node embeddings learned by link prediction via training and evaluating a linear classifier on top of the learned node embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the representations of all the nodes this tutorial uses neighbor sampling as introduced in the [node classification tutorial](L1_large_node_classification.ipynb).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "**Note**: if you would like to obtain node representations without neighbor sampling during inference, please refer to this [user guide](https://docs.dgl.ai/guide/minibatch-inference.html).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, node_features):\n",
    "    with torch.no_grad():\n",
    "        nodes = torch.arange(graph.number_of_nodes())\n",
    "\n",
    "        sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4])\n",
    "        train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "            graph, torch.arange(graph.number_of_nodes()), sampler,\n",
    "            batch_size=1024,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            num_workers=4)\n",
    "\n",
    "        result = []\n",
    "        for input_nodes, output_nodes, bipartites in train_dataloader:\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            result.append(model(bipartites, inputs))\n",
    "\n",
    "        return torch.cat(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def evaluate(emb, label, train_nids, valid_nids, test_nids):\n",
    "    classifier = nn.Linear(emb.shape[1], label.max().item()).cuda()\n",
    "    opt = torch.optim.LBFGS(classifier.parameters())\n",
    "    \n",
    "    def compute_loss():\n",
    "        pred = classifier(emb[train_nids].cuda())\n",
    "        loss = F.cross_entropy(pred, label[train_nids].cuda())\n",
    "        return loss\n",
    "    \n",
    "    def closure():\n",
    "        loss = compute_loss()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    prev_loss = float('inf')\n",
    "    for i in range(1000):\n",
    "        opt.step(closure)\n",
    "        with torch.no_grad():\n",
    "            loss = compute_loss().item()\n",
    "            if np.abs(loss - prev_loss) < 1e-4:\n",
    "                print('Converges at iteration', i)\n",
    "                break\n",
    "            else:\n",
    "                prev_loss = loss\n",
    "                \n",
    "    with torch.no_grad():\n",
    "        pred = classifier(emb.cuda()).cpu()\n",
    "        label = label\n",
    "        valid_acc = sklearn.metrics.accuracy_score(label[valid_nids].numpy(), pred[valid_nids].numpy().argmax(1))\n",
    "        test_acc = sklearn.metrics.accuracy_score(label[test_nids].numpy(), pred[test_nids].numpy().argmax(1))\n",
    "    return valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Training Loop\n",
    "\n",
    "The following initializes the model and defines the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(node_features.shape[1], 128).cuda()\n",
    "predictor = ScorePredictor().cuda()\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the training loop for unsupervised learning and evaluation, and also saves the model that performs the best on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/120819 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/120819 [02:07<4281:11:42, 127.57s/it, loss=89.331]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.7142384863820156 Test Accuracy 0.5608115527106657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1000/120819 [05:41<6:40:05,  4.99it/s, loss=0.937]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|          | 1001/120819 [07:26<1052:04:04, 31.61s/it, loss=0.905]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.5542049182412329 Test Accuracy 0.42179783840791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2000/120819 [10:57<7:12:22,  4.58it/s, loss=0.727]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 2001/120819 [12:33<955:05:57, 28.94s/it, loss=0.734]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.596622841593978 Test Accuracy 0.459599266365459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3000/120819 [16:01<6:49:50,  4.79it/s, loss=0.703]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 3001/120819 [17:38<962:19:21, 29.40s/it, loss=0.697]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.6528749078147649 Test Accuracy 0.5134461258032318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 4000/120819 [21:08<6:32:22,  4.96it/s, loss=0.690]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 4001/120819 [22:44<942:44:40, 29.05s/it, loss=0.663]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.6992854054878824 Test Accuracy 0.5486733261307375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5000/120819 [26:16<7:49:07,  4.11it/s, loss=0.657]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 5001/120819 [27:51<921:38:06, 28.65s/it, loss=0.669]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.7449584212801669 Test Accuracy 0.5972452104319254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 6000/120819 [31:21<7:02:26,  4.53it/s, loss=0.651]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 6001/120819 [32:54<902:48:12, 28.31s/it, loss=0.645]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.7749408742974849 Test Accuracy 0.624674267800104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 7000/120819 [36:27<7:03:32,  4.48it/s, loss=0.645]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 7001/120819 [38:10<984:17:22, 31.13s/it, loss=0.641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.7919792487856979 Test Accuracy 0.6426748832289318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 8000/120819 [41:38<6:14:32,  5.02it/s, loss=0.647]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 8001/120819 [43:25<1012:19:35, 32.30s/it, loss=0.636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.7939373903313582 Test Accuracy 0.6474717939750331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 9000/120819 [46:52<6:13:01,  5.00it/s, loss=0.642]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 9001/120819 [48:19<816:26:21, 26.29s/it, loss=0.641]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8002441319329654 Test Accuracy 0.6522131263468154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 10000/120819 [51:45<6:09:26,  5.00it/s, loss=0.631] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|▊         | 10001/120819 [53:22<894:46:42, 29.07s/it, loss=0.636]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8068051776314117 Test Accuracy 0.6554728205934596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 11000/120819 [56:50<6:14:44,  4.88it/s, loss=0.636]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|▉         | 11001/120819 [58:28<898:17:37, 29.45s/it, loss=0.635]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8087633191770719 Test Accuracy 0.6603591989665134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 12000/120819 [1:02:00<6:00:40,  5.03it/s, loss=0.639]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|▉         | 12001/120819 [1:03:33<846:31:49, 28.01s/it, loss=0.634]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8088904712254914 Test Accuracy 0.6621693369138458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 13000/120819 [1:06:59<6:22:16,  4.70it/s, loss=0.640]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█         | 13001/120819 [1:08:29<813:03:15, 27.15s/it, loss=0.628]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8103145741677898 Test Accuracy 0.664467931955803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 14000/120819 [1:11:58<6:30:34,  4.56it/s, loss=0.626]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 14001/120819 [1:13:32<842:16:26, 28.39s/it, loss=0.632]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.815095491188363 Test Accuracy 0.669851804557517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 15000/120819 [1:17:03<6:04:56,  4.83it/s, loss=0.631]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converges at iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█▏        | 15001/120819 [1:18:34<812:17:23, 27.63s/it, loss=0.640]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Validation Accuracy 0.8154006561045698 Test Accuracy 0.6722141113944252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 15511/120819 [1:20:20<5:52:29,  4.98it/s, loss=0.637]  "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(1):\n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, pos_graph, neg_graph, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            pos_graph = pos_graph.to(torch.device('cuda'))\n",
    "            neg_graph = neg_graph.to(torch.device('cuda'))\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            outputs = model(bipartites, inputs)\n",
    "            pos_score = predictor(pos_graph, outputs)\n",
    "            neg_score = predictor(neg_graph, outputs)\n",
    "            \n",
    "            score = torch.cat([pos_score, neg_score])\n",
    "            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "            \n",
    "            if step % 1000 == 0:\n",
    "                model.eval()\n",
    "                emb = inference(model, graph, node_features)\n",
    "                valid_acc, test_acc = evaluate(emb, node_labels, train_nids, valid_nids, test_nids)\n",
    "                print('Epoch {} Validation Accuracy {} Test Accuracy {}'.format(epoch, valid_acc, test_acc))\n",
    "                if best_accuracy < valid_acc:\n",
    "                    best_accuracy = valid_acc\n",
    "                    torch.save(model.state_dict(), best_model_path)\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned how to train a multi-layer GraphSAGE for unsupervised learning via link prediction on a large dataset that cannot fit into GPU.  The method you have learned can scale to a graph of any size, and works on a single machine with a single GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
