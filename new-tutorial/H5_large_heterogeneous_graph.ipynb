{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The message passing module is identical to what you have seen in the [full graph message passing tutorial](H3_message_passing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule for projecting the input and neighbor feature to the output.\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        \"\"\"Forward computation\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        g : Graph\n",
    "            The input bipartite graph.\n",
    "        h : Tensor\n",
    "            The input node feature.\n",
    "        \"\"\"\n",
    "        with g.local_scope():\n",
    "            h_src, h_dst = h                              # <---\n",
    "            g.srcdata['h'] = h_src                        # <---\n",
    "            # update_all is a message passing API.\n",
    "            g.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h_neigh'))\n",
    "            h_neigh = g.dstdata['h_neigh']\n",
    "            h_total = torch.cat([h_dst, h_neigh], dim=1)  # <---\n",
    "            return self.linear(h_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My problem: currently we recommend users to write the following for message passing with minibatch training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroSAGEConv(nn.Module):\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(HeteroSAGEConv, self).__init__()\n",
    "        \n",
    "        self.conv1 = dglnn.HeteroGraphConv({\n",
    "            'follows': SAGEConv(10, 20),\n",
    "            'plays': SAGEConv(10, 20),\n",
    "            'sells': SAGEConv(10, 20)\n",
    "        }, aggregate='sum')\n",
    "        self.conv2 = dglnn.HeteroGraphConv({\n",
    "            'follows': SAGEConv(20, 20),\n",
    "            'plays': SAGEConv(20, 20),\n",
    "            'sells': SAGEConv(20, 20)\n",
    "        }, aggregate='sum')\n",
    "        \n",
    "    def forward(self, gs, x):\n",
    "        x = self.conv1(gs[0], x)\n",
    "        x = {k: F.relu(v) for k, v in x.items()}\n",
    "        x = self.conv2(gs[1], x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "train_dataloader = dgl.dataloading.NodeDataLoader(\n",
    "    graph, train_nids_dict, sampler,\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "with tqdm.tqdm(train_dataloader) as tq:\n",
    "    for step, (input_nodes, output_nodes, bipartites) in enumerate(tq):\n",
    "        bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "        inputs = {k: node_features[k][v].cuda() for k, v in input_nodes.items()}    # <---\n",
    "        labels = {v: node_labels[k][v].cuda() for k, v in output_nodes.items()}     # <---\n",
    "        predictions = model(bipartites, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will work only if the output nodes of each node type always appear the first in the input nodes of the same node type for each bipartite graph.  The reason is that recall we have the following statement in [large (homogeneous) graph message passing](L4_message_passing.ipynb):\n",
    "\n",
    "```python\n",
    "h_dst = h[:g.number_of_dst_nodes()]           # <---\n",
    "```\n",
    "\n",
    "`dgl.nn.HeteroGraphConv` already does that for you for each node type if it can know that the bipartite graph comes from neighbor sampling (i.e. `dgl.to_block`).\n",
    "\n",
    "```python\n",
    "    def forward(self, g, inputs):\n",
    "        if isinstance(inputs, tuple) or g.is_block:\n",
    "            if isinstance(inputs, tuple):\n",
    "                src_inputs, dst_inputs = inputs\n",
    "            else:\n",
    "                src_inputs = inputs\n",
    "                dst_inputs = {k: v[:g.number_of_dst_nodes(k)] for k, v in inputs.items()}\n",
    "```\n",
    "\n",
    "However, we intend to remove the concept of blocks, so `HeteroGraphConv` will never know if it needs to perform the slicing or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
